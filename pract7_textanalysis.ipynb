{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqLv1KKuLRmk+KlRNlGEGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshita-3104/DSDBAL/blob/main/pract7_textanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obdvyYHdHdSh",
        "outputId": "f700960d-022e-4944-ff07-76a6a764b1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "  tfDict = {}\n",
        "  bagOfWordsCount = len(bagOfWords)\n",
        "  for word, count in wordDict.items():\n",
        "    tfDict[word] = count / float(bagOfWordsCount)\n",
        "  return tfDict"
      ],
      "metadata": {
        "id": "sgiJBfSPMl_j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeIDF(documents):\n",
        "  import math\n",
        "  N = len(documents)\n",
        "  idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "  for document in documents:\n",
        "    for word, val in document.items():\n",
        "      if val > 0:\n",
        "        idfDict[word] += 1\n",
        "        for word, val in idfDict.items():\n",
        "          if val > 0: idfDict[word] = math.log(N / float(val))\n",
        "          else: idfDict[word] = 0\n",
        "  return idfDict"
      ],
      "metadata": {
        "id": "hwByKj3CM4fN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "  tfidf = {}\n",
        "  for word, val in tfBagOfWords.items():\n",
        "    tfidf[word] = val * idfs[word]\n",
        "  return tfidf\n",
        "\n",
        "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
        "print('The given sentences are: \\n', text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNoKaxcANTcI",
        "outputId": "b5b2d6a7-0308-4b92-e200-aabe2f20eb18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given sentences are: \n",
            " Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text= sent_tokenize(text)\n",
        "print(\"\\n Sentence Tokenization: \\n\", tokenized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui3703QfOrHB",
        "outputId": "e849f390-8d28-4a51-ff6b-aa5de97d9aba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sentence Tokenization: \n",
            " ['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "print('\\nWord Tokeniztion: \\n', tokenized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BHyftiyOsj8",
        "outputId": "871e1634-f8bd-4488-f7ef-a21fc992385e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokeniztion: \n",
            " ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "# Removing stop words\n",
        "text= \"How to remove stop words with NLTK library in Python?\"\n",
        "text= re.sub('[^a-zA-Z]', ' ',text)\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered_text=[]\n",
        "for w in tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_text.append(w)\n",
        "print (\"Tokenized Sentence:\",tokens)\n",
        "print (\"Filterd Sentence:\",filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BElM4eUbOxmf",
        "outputId": "b6891902-57d4-4ce4-dc15-54011aa4917f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
            "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words = [\"wait\",\"waited\",\"waiting\",\"waits\"]\n",
        "ps = PorterStemmer()\n",
        "for w in e_words:\n",
        "  rootWord = ps.stem(w)\n",
        "  print(\"Stemming for 'w': \",rootWord)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "text = \"Studies studying cries cry\"\n",
        "wn_lemmatizer = WordNetLemmatizer()\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "  print(\"lemma for {} is {}.\".format(w,wn_lemmatizer.lemmatize(w)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT-xybbsPJPO",
        "outputId": "6cdcaf85-01b4-4bbd-a3d8-9008f51b3ee7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming for 'w':  wait\n",
            "Stemming for 'w':  wait\n",
            "Stemming for 'w':  wait\n",
            "Stemming for 'w':  wait\n",
            "lemma for Studies is Studies.\n",
            "lemma for studying is studying.\n",
            "lemma for cries is cry.\n",
            "lemma for cry is cry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary libraries.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "Xuvs-2kMpXoA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Initialize the Documents.\n",
        "documentA = 'Jupiter is the largest planet'\n",
        "documentB = 'Mars is the fourth planet from the Sun'\n"
      ],
      "metadata": {
        "id": "_jbYodKtpbYh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create BagofWords (BoW) for Document A and B. word tokenization\n",
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')"
      ],
      "metadata": {
        "id": "3fFlXiPupfy9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create Collection of Unique words from Document A and B.\n",
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
      ],
      "metadata": {
        "id": "Z2HcskYwpkhC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create a dictionary of words and their occurrence for each document in the corpus\n",
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "  numOfWordsA[word] += 1 #How many times each word is repeated\n",
        "\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "  numOfWordsB[word] += 1\n"
      ],
      "metadata": {
        "id": "8q7L_wdrpqlS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Compute the term frequency for each of our documents.\n",
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)"
      ],
      "metadata": {
        "id": "Q9pYG7Uzp0R0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Compute the term Inverse Document Frequency.\n",
        "print('----------------Term Frequency----------------------')\n",
        "df = pd.DataFrame([tfA, tfB])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqwdEhpdp_Ko",
        "outputId": "40b03091-a3cb-4156-8c11-e9773e414c8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------Term Frequency----------------------\n",
            "   Jupiter  planet   from    Sun   the  largest   Mars     is  fourth\n",
            "0      0.2   0.200  0.000  0.000  0.20      0.2  0.000  0.200   0.000\n",
            "1      0.0   0.125  0.125  0.125  0.25      0.0  0.125  0.125   0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Compute the term TF/IDF for all words.\n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "print('----------------Inverse Document Frequency----------------------')\n",
        "print(idfs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHUrVwIIqGBE",
        "outputId": "99836268-6aef-487b-ddd7-f9d287da0cc6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------Inverse Document Frequency----------------------\n",
            "{'Jupiter': 4.171783483044043, 'planet': 0, 'from': 1.2800827314091248, 'Sun': 0.5560285979740467, 'the': 0, 'largest': 0.28762552843253275, 'Mars': 0.6351989831585243, 'is': 0, 'fourth': 0.6931471805599453}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "print('------------------- TF-IDF--------------------------------------')\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LafsMrF5qN5S",
        "outputId": "90d5d86d-3af9-479e-d79b-621087e41d0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------- TF-IDF--------------------------------------\n",
            "    Jupiter  planet     from       Sun  the   largest    Mars   is    fourth\n",
            "0  0.834357     0.0  0.00000  0.000000  0.0  0.057525  0.0000  0.0  0.000000\n",
            "1  0.000000     0.0  0.16001  0.069504  0.0  0.000000  0.0794  0.0  0.086643\n"
          ]
        }
      ]
    }
  ]
}